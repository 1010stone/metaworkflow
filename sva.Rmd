# Batch effect

Batch effect is common in high-throughput analysis.

We have a data matrix(M*N) with M stands for indentity peaks from one sample and N stand for individual samples. For one sample, $X = (x_{i1},...,x_{in})^T$ stands for the normalized intensities of peaks. We use $Y = (y_i,...,y_m)^T$ stands for the group infomation of our data. Then we could build such modles:

$$x_{ij} = \mu_i + f_i(y_i) + e_{ij}$$

$\mu_i$ stands for the baseline of the peak intensities in a normal state. Then we have:

$$f_i(y_i) = E(x_{ij}|y_j) - \mu_i$$

stands for the biological variations caused by the our group, for example, whether treated by pollutions or not.

However, considering the batch effects, the real model could be:

$$x_{ij} = \mu_i + f_i(y_i) + \sum_{l = 1}^L \gamma_{li}p_{lj} + e_{ij}^*$$
 $\gamma_{li}$ stands for the peak-specific coefficient for potentical factor $l$. $p_{lj}$ stands for the potential factors across the samples. Actually, the error item $e_{ij}$ in real sample could always be decomposed as $e_{ij} = \sum_{l = 1}^L \gamma_{li}p_{lj} + e_{ij}^*$ with $e_{ij}^*$ standing for the real random error in certain sample for certain peak.
 
We could not get the potential factors directly. Since we don't care the details of the unknown factors, we could estimate orthogonal vectors $h_k$ standing for such potential factors. Thus we have:

$$
x_{ij} = \mu_i + f_i(y_i) + \sum_{l = 1}^L \gamma_{li}p_{lj} + e_{ij}^*\\ 
= \mu_i + f_i(y_i) + \sum_{k = 1}^K \gamma_{ki}h_{kj} + e_{ij}
$$

Here is the details of the algorithm:

> The algorithm is decomposed into two parts: detection of unmodeled factors and construction of surrogate variables

### Detection of unmodeled factors

- Estimate $\hat\mu_i$ and $f_i$ by fitting the model $x_{ij} = \mu_i + f_i(y_i) + e_{ij}$ and get the residual $r_{ij} = x_{ij}-\hat\mu_i - \hat f_i(y_i) $. Then we have the residual matrix R.

- Perform the singular value decompositon(SVD) of the residual matrix $R = UDV^T$

- Let $d_l$ be the $l$th eigenvalue of the diagonal matrix D for $l = 1,...,n$. Set $df$ as the freedom of the model $\hat\mu_i + \hat f_i(y_i)$. We could build a statistic $T_k$ as:

$$T_k = \frac{d_k^2}{\sum_{l=1}^{n-df}d_l^2}$$

to show the variance explained by the $k$th eigenvalue.

- Permute each row of R to remove the structure in the matrix and get $R^*$.

- Fit the model $r_{ij}^* = \mu_i^* + f_i^*(y_i) + e^*_{ij}$ and get $r_{ij}^0 = r^*_{ij}-\hat\mu^*_i - \hat f^*_i(y_i) $ as a null matrix $R_0$

- Perform the singular value decompositon(SVD) of the residual matrix $R_0 = U_0D_0V_0^T$

- Compute the null statistic:

$$
T_k^0 = \frac{d_{0k}^2}{\sum_{l=1}^{n-df}d_{0l}^2}
$$

- Repeat permuting the row B times to get the null statistics $T_k^{0b}$

- Get the p-value for eigengene:

$$p_k = \frac{\#{T_k^{0b}\geq T_k;b=1,...,B }}{B}$$

- For a significance level $\alpha$, treat k as a significant signature of residual R if $p_k\leq\alpha$

### Construction of surrogate variables

- Estimate $\hat\mu_i$ and $f_i$ by fitting the model $x_{ij} = \mu_i + f_i(y_i) + e_{ij}$ and get the residual $r_{ij} = x_{ij}-\hat\mu_i - \hat f_i(y_i) $. Then we have the residual matrix R.

- Perform the singular value decompositon(SVD) of the residual matrix $R = UDV^T$. Let $e_k = (e_{k1},...,e_{kn})^T$ be the $k$th column of V

- Set $\hat K$ as the significant eigenvalues found by the first step.

- Regress each $e_k$ on $x_i$, get the p-value for the association.

- Set $\pi_0$ as the proportion of the peak intensitity $x_i$ not associate with $e_k$ and find the numbers $\hat m =[1-\hat \pi_0 \times m]$ and the indices of the peaks associated with the eigenvalues

- Form the matrix $\hat m_1 \times N$, this matrix$X_r$ stand for the potiential variables. As was done for R, get the eigengents of $X_r$ and denote these by $e_j^r$

- Let $j^* = argmax_{1\leq j \leq n}cor(e_k,e_j^r)$ and set $\hat h_k=e_j^r$. Set the estimate of the surrogate variable to be the eigenvalue of the reduced matrix most correlated with the corresponding residual eigenvalue. Since the reduced matrix is enriched for peaks associated with this residual eigenvalue, this is a principled choice for the estimated surrogate variable that allows for correlation with the primary variable.

- Employ the $ \mu_i + f_i(y_i) + \sum_{k = 1}^K \gamma_{ki}\hat h_{kj} + e_{ij}$ as te estimate of the ideal model $ \mu_i + f_i(y_i) + \sum_{k = 1}^K \gamma_{ki}h_{kj} + e_{ij}$

This method could found the potentical variables for the data. However, such idea could be improved by a iteratively re-weighted least squares approach for estimating surrogate variables. I will show a case study to show such iteratively process. 

### Case study

```{r cache=TRUE,eval=FALSE}
library(faahKO)
source('https://raw.githubusercontent.com/yufree/democode/master/meta/getmetadata.R')
source('https://raw.githubusercontent.com/yufree/democode/master/meta/svaplot.R')
install_github("cbroeckl/RAMClustR")
biocLite('dynamicTreeCut')
biocLite('fastcluster')
biocLite('ff')
path <- system.file("cdf",package = "faahKO")
df <- getdata(path,pmethod='hplcqtof')
lv <- gl(2,6)

df1 <- svameta(df,lv)
df2 <- svameta(df,lv,sva2 = F)

df1a <- df1[df1$pValuesSv<0.05,]
df2a <- df2[df2$pValuesSv<0.05,]

sum(df1a$mzmed %in% df2a$mzmed)

data <- groupval(df,"maxint", value='into')
mod <- model.matrix(~lv)
weights=rep(1,nrow(data))
par(mar = c(4.1, 2.1, 3.5, 2.1), 
    mgp = c(1.5, 0.5, 0))
layout(matrix(c(1:8),nrow=2,byrow=TRUE))
for(b in 1:10){
  svafit <- sva(data,mod,B=b,method = 'irw2')
  svaX <- model.matrix(~lv+svafit$sv)
  lmfit <- lmFit(data,svaX)
  Batch<- lmfit$coef[,(nlevels(lv)+1):(nlevels(lv)+svafit$n.sv)]%*%t(svaX[,(nlevels(lv)+1):(nlevels(lv)+svafit$n.sv)])
  Signal<-lmfit$coef[,1:nlevels(lv)]%*%t(svaX[,1:nlevels(lv)])
  error <- data-Signal-Batch
  colnames(Signal) <- colnames(Batch) <- colnames(error) <- colnames(data)
  ind <- svafit$pprob.gam>0.5&svafit$pprob.b>0.5
  # ind <- svafit$pprob.gam>0
  icolors <- colorRampPalette(rev(brewer.pal(11,"RdYlBu")))(100)
  zlim <- range(c(Signal,data,Batch,error))
  image(t(Signal[ind,]),col=icolors,xlab = 'samples',ylab = 'peaks-signal',xaxt="n",yaxt="n",zlim=zlim)
        axis(1, at=seq(0,1,1/(ncol(data)-1)), labels=colnames(Signal),cex.axis=0.618,las=2)
  image(t(Batch[ind,]),col=icolors,xlab = 'samples',ylab = 'peaks-batch',xaxt="n",yaxt="n",zlim=zlim)
        axis(1, at=seq(0,1,1/(ncol(data)-1)), labels=colnames(Batch),cex.axis=0.618,las=2)
  image(t(error[ind,]),col=icolors,xlab = 'samples',ylab = 'peaks-error',xaxt="n",yaxt="n",zlim=zlim)
        axis(1, at=seq(0,1,1/(ncol(data)-1)), labels=colnames(error),cex.axis=0.618,las=2)
        
  weights = svafit$pprob.gam*(1-svafit$pprob.b)
  surrogate <- svd(data[ind,]*weights[ind])$v[,1]#Weighted SVD
  image(matrix(weights[ind],nrow=1),xaxt="n",yaxt="n",col=brewer.pal(9,"Blues"))
  
  pca <- prcomp(t(Signal), center=TRUE, scale=TRUE) 
  pcab <- prcomp(t(Batch), center=TRUE, scale=TRUE)
  pcae <- prcomp(t(error), center=TRUE, scale=TRUE)
  
  plot(pca$x[,1],pca$x[,2], xlab="PC1", ylab="PC2", pch=colnames(Signal), cex=2, main = "PCA-signal")
  plot(pcab$x[,1],pcab$x[,2], xlab="PC1", ylab="PC2", pch=colnames(Signal), cex=2, main = "PCA-batch")
  plot(pcae$x[,1],pcae$x[,2], xlab="PC1", ylab="PC2", pch=colnames(Signal), cex=2, main = "PCA-error")
  plot(surrogate,pch=21,xlab="",xaxt="n",ylab="Surrogate variable",ylim=c(-.5,.5),cex=1.5)
  abline(v=6.5)
}
b <- ind
a <- ind
ramclustR(df)
```

As you can see from the plot, even after one round the SVA could show a fine result with less effect from batch effect.

Also we might think the unwanted variances in the data could be used for annotation for further studies.